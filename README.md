BTC-Transformer Â· Minuteâ€‘Level Bitcoin Trend Forecast with Transformer
TL;DR: This repository offers a full endâ€‘toâ€‘end workflow â€” from raw Binance Kâ€‘line CSV â†’ feature engineering â†’ automatic labelling â†’ Transformer training & hyperâ€‘parameter tuning â†’ inference & visualisation â€” with CPU/GPU acceleration at every stage.

âœ¨ Highlights
Time2Vector temporal embedding â€” SineActivation first applies a linear map and then concatenates multiâ€‘frequency sine components to expand the original features to hidden_dim, effectively injecting periodic patterns.

Pure encoder architecture â€” only nn.TransformerEncoder is used; no decoder is required for training or inference, which simplifies parallelism and deployment.

Optuna hyperâ€‘parameter search â€” bitcoin_price_prediction_optuna.py explores a 20+ dimensional space with pruning, dynamically balancing CPU/GPU workloads.

Multiâ€‘process feature engineering â€” add_finta_feature_parallel utilises 100+ processes to compute Finta technical indicators, fully saturating the CPU.

Selfâ€‘supervised trend labels â€” detect_trend(_optimized) generates the binary trend_returns label based on drawâ€‘down thresholds, eliminating manual labelling.

ğŸ“‚ Repository Layout
â”œâ”€â”€ bitcoin_price_prediction.py          # Baseline training script (singleâ€‘node)
â”œâ”€â”€ bitcoin_price_prediction_optuna.py   # Hyperâ€‘parameter search + retrain + visualisation
â”œâ”€â”€ model.py                             # BTC_Transformer & Time2Vector
â”œâ”€â”€ evaluation.py                        # Validation / test loops
â”œâ”€â”€ data_process.py                      # Data preprocessing & batch sampling
â”œâ”€â”€ set_target.py                        # Generate trend_returns label
â”œâ”€â”€ reform.py                            # Notebook â†’ .py converter
â””â”€â”€ cuda.py                              # GPU environment check
âš™ï¸ Environment
Component	Version
Python	â‰¥ 3.10
PyTorch	â‰¥ 2.2, CUDA 11.8
pandas / numpy / matplotlib / seaborn	latest
finta	latest
optuna	â‰¥ 3.6
numba / scikit-learn	latest

Installation
conda create -n btc-transformer python=3.10 pytorch cudatoolkit=11.8 -c pytorch -c conda-forge
conda activate btc-transformer
pip install -r requirements.txt
Example requirements.txt:

pandas
numpy
matplotlib
seaborn
scikit-learn
finta
optuna
numba
torchsummary
tqdm
ğŸ“‘ Data Preparation
Download BTCUSDT-1m or BTCUSDT-3m Kâ€‘line CSV from Binance (official) or Kaggle.

Place files under input/btcusdt/ with names like BTCUSDT-1m-2024-12.csv.

Running any training script will automatically:

compute 25+ Finta indicators;

call detect_trend / detect_trend_optimized to create trend_returns;

split train / validation / test in an 8â€¯:â€¯1â€¯:â€¯1 ratio.

ğŸš€ Quick Start
1. Baseline training
python bitcoin_price_prediction.py --epochs 50 --device cuda:0
The script prints training/validation loss curves and saves convergence plots.

2. Hyperâ€‘parameter search
python bitcoin_price_prediction_optuna.py --trials 200 --n_jobs 32
18 dimensions are searched, including Transformer depth, hidden size, learning rate, etc.

CPU/GPU resources are scheduled automatically for maximum throughput.

The best model is stored as best_model_final.pt, with hyperâ€‘parameters in best_params.json.

ğŸ“ Evaluation & Inference
from model import BTC_Transformer
from bitcoin_price_prediction_optuna import define_model
import torch, json

best_params = json.load(open("best_params.json"))
model, _ = define_model(best_params, torch.device("cuda:0"))
model.load_state_dict(torch.load("best_model_final.pt"))
model.eval()
See estimate_BTC for a sample prediction pipeline that returns deâ€‘normalised groundâ€‘truth and forecast series ready for backâ€‘testing.

ğŸ“Š Example Results (3â€‘Minute, 2022â€‘01 â†’ 09)
Metric	Baseline	Optuna Best
Test Loss (CE)	0.51	0.34
â†‘ Directional Accuracy	61â€¯%	69â€¯%

Hardware: single RTX 4090; results for reference only.

ğŸ› ï¸ Customisation & Extensions
Switch to another coin â€” simply replace the CSV; indicators and labelling remain unchanged.

Regression task â€” disable the classifier, enable the generator, and swap the loss to nn.L1Loss().

Multiâ€‘scale windows â€” sample bptt_src / bptt_tgt inside the Optuna objective for stronger generalisation.

Integrate WandB â€” a oneâ€‘liner to track experiments (see TODO).

ğŸ“ TODO
 Integrate Informer or PatchTST to reduce latency

 Use GPUâ€‘Numba/CuPy to accelerate label generation

 WandB experiment tracking & featureâ€‘importance analysis

 Lightweight backâ€‘test engine for profit evaluation
